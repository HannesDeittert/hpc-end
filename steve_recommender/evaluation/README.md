# Evaluation (Agent Benchmark)

Repo-local evaluation pipeline to benchmark **trained RL agents** (checkpoints) on a **fixed anatomy + start/target** setup.

This module intentionally lives **outside** upstream stEVE repos so it can be:
- executed from the **terminal** (CLI),
- imported from the **UI** code,
- and extended without touching upstream stEVE code.

## Quick start (CLI)

Prerequisites are the same as training:

```bash
conda activate master-project
source scripts/sofa_env.sh
```

Edit the example config and set your checkpoint paths:
- `docs/eval_example.yml`

Run:

```bash
steve-eval --config docs/eval_example.yml
```

Outputs:
- `results/eval_runs/<timestamp>_<name>/summary.csv` (per trial)
- `results/eval_runs/<timestamp>_<name>/trials/*.npz` (time series)
- `results/eval_runs/<timestamp>_<name>/report.{md,csv,json}` (per-agent aggregation + ranking)

## Configuration (YAML)

Top-level keys (see `docs/eval_example.yml`):
- `name`: run name (used for output folder)
- `agents`: list of `{name, tool, checkpoint}`
- `n_trials`, `base_seed`
- `max_episode_steps`
- `policy_device`: `"cuda"` or `"cpu"`
- `use_non_mp_sim`: set `true` if you want SOFA force scalars
- `output_root`
- `anatomy`: currently only `type: aortic_arch`
- `scoring`: optional scoring configuration (default is `mode: default_v1`)

## Anatomy / target

Currently supported:
- `aortic_arch` (stEVE built-in generator)
  - start: default `InsertionPoint`
  - target: `BranchEnd` on configured branch end(s), e.g. `["lcca"]`

## Force extraction (important note)

The pipeline records **best-effort scalar proxies** (not a full validated contact force model):
- `wall_lcp_*`: `LCPConstraintSolver.constraintForces` magnitudes
- `wall_wire_force_norm`: norm of `DOFs.force`
- `wall_collision_force_norm`: norm of `CollisionDOFs.force`

These values are only accessible if `use_non_mp_sim: true`, because MP simulation hides the SOFA scene graph.

## Scoring + reports

Each trial gets a scalar `score` plus components:
- `score_success`, `score_efficiency`, `score_safety`, `score_smoothness`

These are aggregated into per-agent ranking reports:
- `report.md` (human-readable)
- `report.csv` (spreadsheet)
- `report.json` (UI / programmatic consumption)

The scoring is intentionally **relative**; force scales are configurable because their absolute units can depend on scene/unit conventions.

## UI integration

You can call the pipeline directly:

```python
from steve_recommender.evaluation import load_config, run_evaluation

cfg = load_config("docs/eval_example.yml")
run_dir = run_evaluation(cfg)
```

Or build a config dict in the UI and convert it:

```python
from steve_recommender.evaluation.config import config_from_dict
cfg = config_from_dict(raw_dict)
```

### UI checkpoint discovery

The UI "Evaluate" tab can auto-discover trained checkpoints if you start training via
`scripts/train_paper.sh`. That wrapper prints a small structured header into
`results/paper_runs/nohup_*.log` which includes `tool=...` and the checkpoint folder path.

The Evaluate tab uses this to:
- filter the tool list to "tools with checkpoints"
- auto-fill a recommended checkpoint (prefers `best_checkpoint.everl`)

## Visual playback (Sofa/Pygame viewer)

You can **watch a trained agent act in the simulator** on a chosen aortic arch via
the small `play_agent.py` helper and an interactive Sofa/Pygame window.

### 1) CLI usage

Example with your Standard‑J wire and the latest checkpoint:

```bash
conda activate master-project
source scripts/sofa_env.sh

python -m steve_recommender.evaluation.play_agent \
  --tool TestModel_StandardJ035/StandardJ035_PTFE \
  --checkpoint results/paper_runs/2025-12-17_111848_paper_standardj/checkpoints/checkpoint8525996.everl \
  --arch-record arch_009999 \
  --device cuda \
  --episodes 1 \
  --max-episode-steps 500
```

Notes:
- `--tool` must match the tool reference used during training (`model/wire`).
- `--checkpoint` is any EveRL checkpoint (`.everl`) from `results/paper_runs/...`.
- `--arch-record` selects an existing aortic arch from the dataset generated by
  `generate_aortic_arch_dataset.py` (e.g. `arch_000123`, `arch_009999`, …).
- `--device` controls the policy device (`cuda` or `cpu`); SOFA itself remains CPU‑based.

The script builds an intervention for the chosen anatomy and wire, wraps it in
`BenchEnv(visualisation=True)` and opens a Sofa/Pygame window.

### 2) UI usage

In the **Evaluate** tab:
- Select an anatomy from the aortic arch dataset.
- Add at least one agent row (tool + checkpoint).
- Click **“Play first agent (Sofa window)”**.

The UI spawns `play_agent.py` in a separate process with the selected anatomy and the
first configured agent; logs appear in the Evaluate tab while the Sofa window is open.

### 3) Camera controls in the Sofa window

The viewer uses an interactive wrapper around `eve.visualisation.SofaPygame`
(`InteractiveSofaPygame`) with simple keyboard controls:

- Arrow left / right: rotate LAO / RAO (around z‑axis)
- Arrow up / down: rotate CRA / CAU (around x‑axis)
- `W`: zoom in
- `S`: zoom out

Tips:
- Hold keys to change the view continuously while the agent runs.
- Close the window or press `Ctrl+C` in the terminal to stop playback early.

## More docs

See `docs/evaluation_pipeline.md` for full details on outputs and file formats.
